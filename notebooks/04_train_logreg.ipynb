{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (env, Spark, load features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/28 16:46:01 WARN Utils: Your hostname, JINUTSA, resolves to a loopback address: 127.0.1.1; using 10.4.8.103 instead (on interface enp37s0f0)\n",
      "25/10/28 16:46:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/28 16:46:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/28 16:46:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features rows: 1163\n",
      "root\n",
      " |-- patient_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- ethnicity: string (nullable = true)\n",
      " |-- age_at_index: decimal(13,0) (nullable = true)\n",
      " |-- index_date: date (nullable = true)\n",
      " |-- last_enc_date: date (nullable = true)\n",
      " |-- n_encounters: long (nullable = true)\n",
      " |-- n_conditions: long (nullable = true)\n",
      " |-- n_procedures: long (nullable = true)\n",
      " |-- n_medications: long (nullable = true)\n",
      " |-- n_observations: long (nullable = true)\n",
      " |-- n_claims: long (nullable = true)\n",
      " |-- hist_total_cost: double (nullable = true)\n",
      " |-- n_unique_providers: long (nullable = true)\n",
      " |-- n_unique_departments: long (nullable = true)\n",
      " |-- n_claims_with_diag: long (nullable = true)\n",
      " |-- claim_span_days: integer (nullable = true)\n",
      " |-- cost_next_window: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "+------------------------------------+------+-----+-----------+------------+----------+-------------+------------+------------+------------+-------------+--------------+--------+---------------+------------------+--------------------+------------------+---------------+----------------+-----+\n",
      "|patient_id                          |gender|race |ethnicity  |age_at_index|index_date|last_enc_date|n_encounters|n_conditions|n_procedures|n_medications|n_observations|n_claims|hist_total_cost|n_unique_providers|n_unique_departments|n_claims_with_diag|claim_span_days|cost_next_window|label|\n",
      "+------------------------------------+------+-----+-----------+------------+----------+-------------+------------+------------+------------+-------------+--------------+--------+---------------+------------------+--------------------+------------------+---------------+----------------+-----+\n",
      "|c9b4f8c6-d7be-6563-0560-c90cfe6b12ca|F     |white|nonhispanic|22          |2020-11-11|2021-11-11   |83          |5           |95          |6            |169           |89      |910383.61      |2                 |4                   |89                |5950           |14386.52        |1    |\n",
      "|8761af66-90ad-7f7d-2102-58b767495473|M     |white|nonhispanic|34          |2020-11-05|2021-11-05   |15          |17          |25          |4            |183           |19      |7772.72        |3                 |5                   |19                |5266           |269.68          |0    |\n",
      "|a5ca099b-c28d-c0f4-653a-0d926051840a|M     |white|nonhispanic|6           |2020-08-05|2021-08-05   |5           |0           |3           |0            |64            |5       |4705.33        |1                 |1                   |5                 |252            |2797.57         |0    |\n",
      "|5136cc20-a63c-c7c8-00ae-fc5fa86be863|M     |black|nonhispanic|83          |2020-03-25|2021-03-25   |50          |64          |60          |3            |367           |53      |104101.03      |3                 |4                   |53                |21651          |17280.38        |1    |\n",
      "|d7b59f8c-fe04-46a1-de11-1ca3bd0d58e9|M     |black|nonhispanic|26          |2020-03-16|2021-03-16   |17          |15          |52          |6            |244           |23      |13200.5        |2                 |4                   |23                |2969           |1543.7          |0    |\n",
      "+------------------------------------+------+-----+-----------+------------+----------+-------------+------------+------------+------------+-------------+--------------+--------+---------------+------------------+--------------------+------------------+---------------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Env & paths\n",
    "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "assert DATA_DIR and os.path.isdir(DATA_DIR), f\"DATA_DIR invalid: {DATA_DIR}\"\n",
    "\n",
    "# 2) Spark (used only to load parquet and convert to pandas)\n",
    "spark = SparkSession.builder.appName(\"HealthClaims_TrainXGB\").getOrCreate()\n",
    "\n",
    "# 3) Load features saved by 02_label_features\n",
    "feat_dir = os.path.abspath(os.path.join(DATA_DIR, \"..\", \"processed\", \"features_parquet\"))\n",
    "features = spark.read.parquet(feat_dir).cache()\n",
    "features.createOrReplaceTempView(\"features_v0\")\n",
    "\n",
    "print(\"Features rows:\", features.count())\n",
    "features.printSchema()\n",
    "features.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-based split (index_date) & pandas conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 914  Test rows: 249\n",
      "Train label counts:\n",
      " label\n",
      "0    662\n",
      "1    252\n",
      "Name: count, dtype: int64\n",
      "Test  label counts:\n",
      " label\n",
      "0    152\n",
      "1     97\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Time split (mimics production): train on earlier index dates, test on later ones\n",
    "CUT = \"2020-10-15\"\n",
    "train_df = spark.sql(f\"SELECT * FROM features_v0 WHERE index_date < DATE('{CUT}')\")\n",
    "test_df  = spark.sql(f\"SELECT * FROM features_v0 WHERE index_date >= DATE('{CUT}')\")\n",
    "\n",
    "print(\"Train rows:\", train_df.count(), \" Test rows:\", test_df.count())\n",
    "\n",
    "# Columns we’ll use (v0 features)\n",
    "CAT = [\"gender\",\"race\",\"ethnicity\"]\n",
    "NUM = [\n",
    "    \"age_at_index\", \"n_encounters\", \"n_conditions\", \"n_procedures\", \"n_medications\",\n",
    "    \"n_observations\", \"n_claims\", \"hist_total_cost\",\n",
    "    \"n_unique_providers\", \"n_unique_departments\", \"claim_span_days\"\n",
    "]\n",
    "TARGET = \"label\"\n",
    "\n",
    "train_pd = train_df.select(*(CAT + NUM + [TARGET])).toPandas()\n",
    "test_pd  = test_df.select(*(CAT + NUM + [TARGET])).toPandas()\n",
    "\n",
    "# Sanity on class balance (train/test)\n",
    "print(\"Train label counts:\\n\", train_pd[TARGET].value_counts(dropna=False))\n",
    "print(\"Test  label counts:\\n\", test_pd[TARGET].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding, alignment, and class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight (neg/pos): 2.63 (neg=662, pos=252)\n"
     ]
    }
   ],
   "source": [
    "# Categoricals → category dtype\n",
    "for c in CAT:\n",
    "    train_pd[c] = train_pd[c].astype(\"category\")\n",
    "    test_pd[c]  = test_pd[c].astype(\"category\")\n",
    "\n",
    "# One-hot (drop_first avoids perfect multicollinearity)\n",
    "X_train = pd.get_dummies(train_pd[CAT + NUM], drop_first=True)\n",
    "X_test  = pd.get_dummies(test_pd[CAT + NUM], drop_first=True)\n",
    "\n",
    "# Align columns (add missing columns to test, in same order as train)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "y_train = train_pd[TARGET].astype(int).values\n",
    "y_test  = test_pd[TARGET].astype(int).values\n",
    "\n",
    "# Optional: drop any constant columns (rare but possible)\n",
    "const_cols = [c for c in X_train.columns if X_train[c].nunique() <= 1]\n",
    "if const_cols:\n",
    "    X_train = X_train.drop(columns=const_cols)\n",
    "    X_test  = X_test.drop(columns=const_cols)\n",
    "    print(\"Dropped constant columns:\", const_cols)\n",
    "\n",
    "# Compute scale_pos_weight to handle imbalance: (#neg / #pos) on train\n",
    "import numpy as np\n",
    "pos = np.sum(y_train == 1)\n",
    "neg = np.sum(y_train == 0)\n",
    "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
    "print(f\"scale_pos_weight (neg/pos): {scale_pos_weight:.2f} (neg={neg}, pos={pos})\")\n",
    "\n",
    "# Keep metadata for inference\n",
    "feature_columns = X_train.columns.tolist()\n",
    "cat_snapshot = {c: list(train_pd[c].cat.categories) for c in CAT}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI set to: file:///home/utsajinlab/health_claims_ml/notebooks/mlruns_clean\n",
      "\n",
      "C=0.01\n",
      "Test Precision: 0.509\n",
      "Test Recall:    0.557\n",
      "Test F1:        0.532\n",
      "Test ROC-AUC:   0.638\n",
      "Test PR-AUC:    0.512\n",
      "\n",
      "C=0.1\n",
      "Test Precision: 0.531\n",
      "Test Recall:    0.536\n",
      "Test F1:        0.533\n",
      "Test ROC-AUC:   0.627\n",
      "Test PR-AUC:    0.522\n",
      "\n",
      "C=1.0\n",
      "Test Precision: 0.532\n",
      "Test Recall:    0.515\n",
      "Test F1:        0.524\n",
      "Test ROC-AUC:   0.624\n",
      "Test PR-AUC:    0.527\n",
      "\n",
      "C=10.0\n",
      "Test Precision: 0.533\n",
      "Test Recall:    0.505\n",
      "Test F1:        0.519\n",
      "Test ROC-AUC:   0.625\n",
      "Test PR-AUC:    0.529\n",
      "\n",
      " All runs complete. Check MLflow UI or printed results for best precision/F1.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlflow.tracking import MlflowClient\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import mlflow\n",
    "\n",
    "# -------------------- Scaling --------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------- MLflow Setup --------------------\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "mlruns_clean_path = os.path.join(project_root, \"mlruns_clean\")\n",
    "os.makedirs(mlruns_clean_path, exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file://{mlruns_clean_path}\")\n",
    "print(\"Tracking URI set to:\", mlflow.get_tracking_uri())\n",
    "\n",
    "experiment_name = \"health_claims_highcost_logreg\"\n",
    "client = MlflowClient()\n",
    "if not client.get_experiment_by_name(experiment_name):\n",
    "    client.create_experiment(name=experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# -------------------- Cross-validation setup --------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "C_values = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "# -------------------- Run experiments --------------------\n",
    "for i, C in enumerate(C_values):\n",
    "    with mlflow.start_run(run_name=f\"logreg_run_cv_{i}\"):\n",
    "        mlflow.set_tag(\"mlflow.runName\", f\"logreg_run_cv_{i}\")\n",
    "        mlflow.log_param(\"C\", C)\n",
    "        mlflow.log_param(\"class_weight\", \"balanced\")\n",
    "\n",
    "        model = LogisticRegression(\n",
    "            C=C,\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=1000,\n",
    "            solver=\"lbfgs\",\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Cross-validation with added metrics\n",
    "        scoring = {\n",
    "            \"roc_auc\": \"roc_auc\",\n",
    "            \"pr_auc\": \"average_precision\",\n",
    "            \"precision\": \"precision\",\n",
    "            \"recall\": \"recall\",\n",
    "            \"f1\": \"f1\"\n",
    "        }\n",
    "\n",
    "        scores = cross_validate(\n",
    "            model,\n",
    "            X_train_scaled, y_train,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            return_train_score=False\n",
    "        )\n",
    "\n",
    "        # Log CV metrics\n",
    "        for metric in scoring.keys():\n",
    "            mlflow.log_metric(f\"cv_{metric}\", scores[f\"test_{metric}\"].mean())\n",
    "\n",
    "        # -------------------- Fit and evaluate on test set --------------------\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        proba_test = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        pred_test = (proba_test >= 0.5).astype(int)\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, proba_test)\n",
    "        pr_auc = average_precision_score(y_test, proba_test)\n",
    "        precision = precision_score(y_test, pred_test)\n",
    "        recall = recall_score(y_test, pred_test)\n",
    "        f1 = f1_score(y_test, pred_test)\n",
    "\n",
    "        mlflow.log_metric(\"test_roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"test_pr_auc\", pr_auc)\n",
    "        mlflow.log_metric(\"test_precision\", precision)\n",
    "        mlflow.log_metric(\"test_recall\", recall)\n",
    "        mlflow.log_metric(\"test_f1\", f1)\n",
    "\n",
    "        print(f\"\\nC={C}\")\n",
    "        print(f\"Test Precision: {precision:.3f}\")\n",
    "        print(f\"Test Recall:    {recall:.3f}\")\n",
    "        print(f\"Test F1:        {f1:.3f}\")\n",
    "        print(f\"Test ROC-AUC:   {roc_auc:.3f}\")\n",
    "        print(f\"Test PR-AUC:    {pr_auc:.3f}\")\n",
    "\n",
    "        # -------------------- Confusion Matrix --------------------\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, pred_test, ax=ax)\n",
    "        plt.title(f\"Confusion Matrix (C={C})\")\n",
    "        cm_path = f\"conf_matrix_C{C}.png\"\n",
    "        plt.savefig(cm_path)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "        plt.close()\n",
    "\n",
    "print(\"\\n All runs complete. Check MLflow UI or printed results for best precision/F1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Logistic Regression MLflow Results (sorted by test F1) =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>C</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>cv_precision</th>\n",
       "      <th>cv_recall</th>\n",
       "      <th>cv_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>test_pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg_run_cv_1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logreg_run_cv_0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg_run_cv_2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg_run_cv_3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          run_name     C class_weight  cv_precision  cv_recall  cv_f1  \\\n",
       "0  logreg_run_cv_1   0.1     balanced         0.458      0.619  0.526   \n",
       "1  logreg_run_cv_0  0.01     balanced         0.447      0.602  0.512   \n",
       "2  logreg_run_cv_2   1.0     balanced         0.450      0.607  0.517   \n",
       "3  logreg_run_cv_3  10.0     balanced         0.440      0.591  0.504   \n",
       "\n",
       "   test_precision  test_recall  test_f1  test_roc_auc  test_pr_auc  \n",
       "0           0.531        0.536    0.533         0.627        0.522  \n",
       "1           0.509        0.557    0.532         0.638        0.512  \n",
       "2           0.532        0.515    0.524         0.624        0.527  \n",
       "3           0.533        0.505    0.519         0.625        0.529  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Load experiment data --------------------\n",
    "experiment_name = \"health_claims_highcost_logreg\"\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if experiment is None:\n",
    "    raise ValueError(f\"Experiment '{experiment_name}' not found. Check MLflow tracking URI.\")\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.test_f1 DESC\"]\n",
    ")\n",
    "\n",
    "# -------------------- Extract relevant info --------------------\n",
    "records = []\n",
    "for run in runs:\n",
    "    metrics = run.data.metrics\n",
    "    params = run.data.params\n",
    "\n",
    "    record = {\n",
    "        \"run_name\": run.info.run_name,\n",
    "        \"C\": params.get(\"C\"),\n",
    "        \"class_weight\": params.get(\"class_weight\"),\n",
    "        \"cv_precision\": metrics.get(\"cv_precision\"),\n",
    "        \"cv_recall\": metrics.get(\"cv_recall\"),\n",
    "        \"cv_f1\": metrics.get(\"cv_f1\"),\n",
    "        \"test_precision\": metrics.get(\"test_precision\"),\n",
    "        \"test_recall\": metrics.get(\"test_recall\"),\n",
    "        \"test_f1\": metrics.get(\"test_f1\"),\n",
    "        \"test_roc_auc\": metrics.get(\"test_roc_auc\"),\n",
    "        \"test_pr_auc\": metrics.get(\"test_pr_auc\")\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "df_results = pd.DataFrame(records)\n",
    "\n",
    "# -------------------- Sort and format --------------------\n",
    "df_results = df_results.sort_values(by=\"test_f1\", ascending=False).reset_index(drop=True)\n",
    "df_results = df_results.round(3)\n",
    "\n",
    "# Display neatly\n",
    "print(\"\\n===== Logistic Regression MLflow Results (sorted by test F1) =====\")\n",
    "display(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (health_claims_ml)",
   "language": "python",
   "name": "health_claims_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
